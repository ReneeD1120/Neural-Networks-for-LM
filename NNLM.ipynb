{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YO-CIIURnClb"
      },
      "id": "YO-CIIURnClb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1355dfd",
      "metadata": {
        "id": "b1355dfd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "import re\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba"
      ],
      "metadata": {
        "id": "ASprXMxa1JGC"
      },
      "id": "ASprXMxa1JGC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4061bd12",
      "metadata": {
        "id": "4061bd12"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/英文文档NLP/data.txt','r',encoding='utf-8') as file:\n",
        "    content=file.readlines()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cd23fa",
      "metadata": {
        "id": "10cd23fa"
      },
      "outputs": [],
      "source": [
        "def preprocessing(sent):\n",
        "    pattern = re.compile(r'([\\u4e00-\\u9fa5])')\n",
        "    chars = \"\".join(pattern.findall(sent))\n",
        "    #chars =[w for w in chars if len(w) > 0]\n",
        "    return chars"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9v6jKJEXQoy5"
      },
      "id": "9v6jKJEXQoy5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4bb6128",
      "metadata": {
        "id": "e4bb6128"
      },
      "outputs": [],
      "source": [
        "def dic(content):\n",
        "  #sentences=preprocessing(content)\n",
        "  \n",
        "  word_list=preprocessing(content)\n",
        "  #word_list=jieba.lcut(word_list)\n",
        "  word_list = ' '.join(word_list).split()\n",
        "  word_list = list(set(word_list))\n",
        "\n",
        "  word2id = {w: i for i, w in enumerate(word_list)}\n",
        "  id2word = {i: w for i, w in enumerate(word_list)}\n",
        "  return word2id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/英文文档NLP/data.txt','r',encoding='utf-8') as file:\n",
        "    content_total=file.read()\n",
        "    word2id=dic(content_total)"
      ],
      "metadata": {
        "id": "JRqP4ZizMcNs"
      },
      "id": "JRqP4ZizMcNs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4fffb35",
      "metadata": {
        "id": "c4fffb35"
      },
      "outputs": [],
      "source": [
        "def computeCE(sen,word2id):\n",
        "\n",
        "    \n",
        "    \n",
        "  #sentences = ['我 喜欢 狗', '我 讨厌 牛奶', '我 做 自然语言处理', '我 看 电影']\n",
        "  sentences = sen\n",
        "  \n",
        "  #word_list = ' '.join(sentences).split()\n",
        "  #word_list = list(set(word_list))\n",
        "\n",
        "  #word2id = {w: i for i, w in enumerate(word_list)}\n",
        "  #id2word = {i: w for i, w in enumerate(word_list)}\n",
        "\n",
        "  def make_batch(sentence):\n",
        "      input_batch = []\n",
        "      target_batch = []\n",
        "      #word = sentence.split()\n",
        "      \n",
        "      for sen in sentence:\n",
        "        #word=jieba.lcut(sen)\n",
        "        word=' '.join(sen).split()\n",
        "        for n in range(len(word)-5):\n",
        "          inputs = [word2id[w] for w in word[n:n+5]]\n",
        "          target = word2id[word[n+5]]\n",
        "          input_batch.append(inputs)\n",
        "          target_batch.append(target)\n",
        "      #print(input_batch,target_batch)\n",
        "      return input_batch, target_batch\n",
        "\n",
        "  input_batch, target_batch = make_batch(sentences)\n",
        "  input_batch = torch.LongTensor(input_batch)\n",
        "  target_batch = torch.LongTensor(target_batch)\n",
        "\n",
        "\n",
        "  n_class = len(word2id) # 词表大小\n",
        "  n_hidden = 10   # 隐藏层神经单元数目\n",
        "  m = 5  # 词向量维度\n",
        "  n_step = 5   # 代表n-gram模型 2就是2-gram 用前两个词预测最后一个词\n",
        "  # 搭建模型\n",
        "  class NNLM(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(NNLM, self).__init__()\n",
        "          self.embd = nn.Embedding(n_class, m)\n",
        "          self.W = nn.Parameter(torch.randn(n_step*m, n_hidden))\n",
        "          self.b = nn.Parameter(torch.randn(n_hidden))\n",
        "          self.U = nn.Parameter(torch.randn(n_hidden, n_class))\n",
        "          self.d = nn.Parameter(torch.randn(n_class))\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = self.embd(x)\n",
        "          x = x.view(-1, n_step*m)\n",
        "          tanh = torch.tanh(torch.mm(x, self.W) + self.b)\n",
        "          output = torch.mm(tanh, self.U) + self.d\n",
        "\n",
        "          return output\n",
        "\n",
        "  model = NNLM()  # 模型实例化\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()  # 选择损失函数\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)  # 选择优化器\n",
        "\n",
        "  # 迭代训练\n",
        "  for i in range(1000):\n",
        "      optimizer.zero_grad()  # 梯度清零\n",
        "\n",
        "      output = model(input_batch)\n",
        "      \n",
        "      loss = criterion(output, target_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "  #predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
        "  #print([sen.split()[:-1] for sen in sentences], '-->', [id2word[n.item()] for n in predict.squeeze()])\n",
        "      \n",
        "  return loss,model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c870bee",
      "metadata": {
        "id": "0c870bee"
      },
      "outputs": [],
      "source": [
        "ppltotal=0.\n",
        "count=0\n",
        "sen1=[]\n",
        "for i in range(len(content)):\n",
        "  sen=preprocessing(content[i])\n",
        "  if len(sen)>5:\n",
        "    sen1.append(sen)\n",
        "train_data=sen1[0:20000]\n",
        "    #if math.isnan(CE)==False:\n",
        "      #ppltotal+=pow(2,CE)\n",
        "      #count+=1\n",
        "  #else:\n",
        "    #break\n",
        "    #pr\n",
        "    \n",
        "    #int(i)\n",
        "#PPL=ppltotal/count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CE,model=computeCE(train_data,word2id)\n",
        "\n",
        "print(CE.item())"
      ],
      "metadata": {
        "id": "152cHX-wF9TU"
      },
      "id": "152cHX-wF9TU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(sentence):\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "      #word = sentence.split()\n",
        "      \n",
        "    for sen in sentence:\n",
        "        #word=jieba.lcut(sen)\n",
        "      word=' '.join(sen).split()\n",
        "      for n in range(len(word)-5):\n",
        "        inputs = [word2id[w] for w in word[n:n+5]]\n",
        "        target = word2id[word[n+5]]\n",
        "        input_batch.append(inputs)\n",
        "        target_batch.append(target)\n",
        "      #print(input_batch,target_batch)\n",
        "    return input_batch, target_batch"
      ],
      "metadata": {
        "id": "q3xsQHfvB3UD"
      },
      "id": "q3xsQHfvB3UD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata=sen1[20000:30000]\n",
        "loss=[]\n",
        "for test_data in testdata:\n",
        "  input_batch, target_batch = make_batch(test_data)\n",
        "  input_batch = torch.LongTensor(input_batch)\n",
        "  target_batch = torch.LongTensor(target_batch)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  l=criterion(model(input_batch),target_batch)\n",
        "  print(l)\n",
        "  loss.append(l)"
      ],
      "metadata": {
        "id": "-bJZlDBiGZHq"
      },
      "id": "-bJZlDBiGZHq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata=sen1[20000:50000]\n",
        "loss=[]\n",
        "\n",
        "input_batch, target_batch = make_batch(testdata)\n",
        "input_batch = torch.LongTensor(input_batch)\n",
        "target_batch = torch.LongTensor(target_batch)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "l=criterion(model(input_batch),target_batch)\n",
        "print(l)\n",
        "loss.append(l)"
      ],
      "metadata": {
        "id": "UZHlImOId7-P"
      },
      "id": "UZHlImOId7-P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppl=sum(np.exp(loss[i].item()) for i in range(len(loss)))"
      ],
      "metadata": {
        "id": "GCa_dKbxDvtL"
      },
      "id": "GCa_dKbxDvtL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppl"
      ],
      "metadata": {
        "id": "9bpumQcEebbv"
      },
      "id": "9bpumQcEebbv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "id": "krlibj8X6gXe"
      },
      "id": "krlibj8X6gXe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),'/content/drive/MyDrive/英文文档NLP/NNLM.pkl' )"
      ],
      "metadata": {
        "id": "mTHSgpomsaV7"
      },
      "id": "mTHSgpomsaV7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "computeCE(preprocessing(content[2]))+computeCE(preprocessing(content[0]))"
      ],
      "metadata": {
        "id": "atBUY3E16KSP"
      },
      "id": "atBUY3E16KSP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}