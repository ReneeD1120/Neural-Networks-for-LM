{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "-rCAB9GS9kGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLWQ3hpkzJ7i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QGw-2rOw6tdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def computeppl(sen):\n",
        "    with torch.no_grad():\n",
        "        model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
        "        model.eval()\n",
        "        # Load pre-trained model tokenizer (vocabulary)\n",
        "        tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
        "        sentence = sen\n",
        "        tokenize_input = tokenizer.tokenize(sentence)\n",
        "        tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
        "        sen_len = len(tokenize_input)\n",
        "        sentence_loss = 0.\n",
        "\n",
        "        for i, word in enumerate(tokenize_input):\n",
        "            # add mask to i-th character of the sentence\n",
        "            tokenize_input[i] = '[MASK]'\n",
        "            mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
        "\n",
        "            output = model(mask_input)\n",
        "\n",
        "            prediction_scores = output[0]\n",
        "            softmax = nn.Softmax(dim=0)\n",
        "            ps = softmax(prediction_scores[0, i]).log()\n",
        "            word_loss = ps[tensor_input[0, i]]\n",
        "            sentence_loss += word_loss.item()\n",
        "\n",
        "            tokenize_input[i] = word\n",
        "        ppl = np.exp(-sentence_loss/sen_len)\n",
        "    return(ppl)"
      ],
      "metadata": {
        "id": "c0iQdkURzRLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def getText(fileName):\n",
        "    all_text = \"\"\n",
        "    file_object = open(fileName, 'r',encoding = 'utf-8')\n",
        "    try:\n",
        "        all_text += file_object.read()\n",
        "    finally:\n",
        "        file_object.close()\n",
        "    print(all_text)\n",
        "    re_patten = '[^\\u4e00-\\u9fa5？。，！]'\n",
        "    all_text = re.sub(re_patten, \"\", all_text.strip())\n",
        "    #reChinese = re.compile('/[(\\u4e000-\\u9fff)(A-Za-z0-9)(，,（）【】{}！\\-!)]+/g ') \n",
        "    #all_text = reChinese.findall(all_text) \n",
        "    #all_text = all_text.split()\n",
        "    #for item in all_text:\n",
        "        #print(item)\n",
        "    return all_text\n"
      ],
      "metadata": {
        "id": "OBL3KNY0zXh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=getText('/content/drive/MyDrive/英文文档NLP/data.txt')"
      ],
      "metadata": {
        "id": "PSVzXwbK0K7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/英文文档NLP/data.txt'"
      ],
      "metadata": {
        "id": "XVg04lkNzd4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(path,\"r\",encoding='utf-8')\n",
        "a=[]\n",
        "text = f.read().split()\n",
        "for line in text:\n",
        "    if line == '“'or line == '”'or line == '：'or line == '-' or line == '\"' or line == '？'or line == '、' or line == '）'or line == '（' or line == '！':\n",
        "        continue\n",
        "    else:\n",
        "        a.append(line)"
      ],
      "metadata": {
        "id": "-ycgvGuRzhoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "id": "nTsehGh16-lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppltotal=0\n",
        "count=0\n",
        "for i in a[0:10000]:  \n",
        "    print(i)\n",
        "    ppltotal+=computeppl(i)\n",
        "    count+=1\n",
        "    print(count)\n",
        "    print(ppltotal)"
      ],
      "metadata": {
        "id": "7QOtv2mQzk1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppl=ppltotal/count"
      ],
      "metadata": {
        "id": "yETBRazBW-Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppl"
      ],
      "metadata": {
        "id": "4TthNoZPXJ0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}